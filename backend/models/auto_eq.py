# -*- coding: utf-8 -*-
"""Auto-EQ

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xu8wkegTSxCOo9mKiH4GPy1Kb39ldPGP
"""

from google.colab import drive
drive.mount('/content/drive')

import os

BASE_DIR = "/content/drive/MyDrive/project-music/data"

print("BASE_DIR:", BASE_DIR)
print("ภายใน data มีอะไรบ้าง:")
print(os.listdir(BASE_DIR))

print("\nภายใน mix_tracks:")
print(os.listdir(os.path.join(BASE_DIR, "mix_tracks")))

import os
import librosa
import numpy as np

BASE_DIR   = "/content/drive/MyDrive/project-music"
RAW_ROOT   = os.path.join(BASE_DIR, "data/raw_tracks")
MIX_ROOT   = os.path.join(BASE_DIR, "data/mix_tracks")
DATASET_DIR = os.path.join(BASE_DIR, "datasets")
os.makedirs(DATASET_DIR, exist_ok=True)

GENRES = ["trap", "pop", "rock", "soul", "country"]

SAMPLE_RATE = 44100
SEGMENT_SECONDS = 5
SEGMENT_SAMPLES = SAMPLE_RATE * SEGMENT_SECONDS

# โหลดเป็น "คู่" โดยตัดให้ยาวเท่ากันตาม min_len
def load_segments_pair(raw_path, mix_path, max_segments=20):
    raw_y, sr1 = librosa.load(raw_path, sr=SAMPLE_RATE, mono=True)
    mix_y, sr2 = librosa.load(mix_path, sr=SAMPLE_RATE, mono=True)

    assert sr1 == sr2 == SAMPLE_RATE

    # ใช้เฉพาะช่วงที่ซ้อนทับกัน
    min_len = min(len(raw_y), len(mix_y))
    raw_y = raw_y[:min_len]
    mix_y = mix_y[:min_len]

    if min_len < SEGMENT_SAMPLES:
        return [], []

    segments_raw = []
    segments_mix = []

    num_segments = min(max_segments, min_len // SEGMENT_SAMPLES)
    for i in range(num_segments):
        start = i * SEGMENT_SAMPLES
        end   = start + SEGMENT_SAMPLES
        segments_raw.append(raw_y[start:end])
        segments_mix.append(mix_y[start:end])

    return segments_raw, segments_mix


X = []           # raw
Y = []           # mix/master
genre_labels = []

for genre in GENRES:
    raw_folder = os.path.join(RAW_ROOT, genre)
    mix_folder = os.path.join(MIX_ROOT, genre)

    raw_files = [f for f in os.listdir(raw_folder) if f.endswith(".wav")]
    mix_files = [f for f in os.listdir(mix_folder) if f.endswith(".wav")]

    common = sorted(list(set(raw_files) & set(mix_files)))
    print(f"\nGenre {genre}: มีคู่ไฟล์ {len(common)} เพลง")

    for fname in common:
        raw_path = os.path.join(raw_folder, fname)
        mix_path = os.path.join(mix_folder, fname)

        raw_segs, mix_segs = load_segments_pair(raw_path, mix_path, max_segments=20)

        for r_seg, m_seg in zip(raw_segs, mix_segs):
            X.append(r_seg.astype(np.float32))
            Y.append(m_seg.astype(np.float32))
            genre_labels.append(genre)

X = np.stack(X, axis=0)
Y = np.stack(Y, axis=0)
genre_labels = np.array(genre_labels)

print("\nX shape:", X.shape)   # (N, 220500)
print("Y shape:", Y.shape)
print("ตัวอย่าง labels:", genre_labels[:10])

SAVE_PATH = os.path.join(DATASET_DIR, "autoeq_train_rawmix_v1.npz")
np.savez(SAVE_PATH, X=X, Y=Y, genre=genre_labels)
print("Saved dataset to:", SAVE_PATH)

data = np.load(SAVE_PATH, allow_pickle=True)
genres = data["genre"]
unique, counts = np.unique(genres, return_counts=True)
print("Unique labels:", unique)
print("Counts:", counts)

import os
import numpy as np
import librosa

BASE_DIR     = "/content/drive/MyDrive/project-music"
DATASET_DIR  = os.path.join(BASE_DIR, "datasets")
IN_PATH      = os.path.join(DATASET_DIR, "autoeq_train_rawmix_v1.npz")
OUT_PATH     = os.path.join(DATASET_DIR, "autoeq_mel_rawmix_v1.npz")

data = np.load(IN_PATH, allow_pickle=True)
X_wave = data["X"]      # raw
Y_wave = data["Y"]      # mix/master
genres = data["genre"]

print("X_wave:", X_wave.shape)
print("Y_wave:", Y_wave.shape)

# Mel-spec settings
sr        = 44100
n_fft     = 2048
hop       = 512
n_mels    = 128

def to_mel_db(wave):
    mel = librosa.feature.melspectrogram(
        y=wave,
        sr=sr,
        n_fft=n_fft,
        hop_length=hop,
        n_mels=n_mels,
        power=2.0
    )
    mel_db = librosa.power_to_db(mel, ref=np.max)
    return mel_db.astype(np.float32)

X_mel_list = []
Y_mel_list = []

for i in range(len(X_wave)):
    X_mel_list.append(to_mel_db(X_wave[i]))
    Y_mel_list.append(to_mel_db(Y_wave[i]))

X_mel = np.stack(X_mel_list, axis=0)  # (N, 128, T)
Y_mel = np.stack(Y_mel_list, axis=0)

print("X_mel shape:", X_mel.shape)
print("Y_mel shape:", Y_mel.shape)

# เซฟเป็นไฟล์ใหม่สำหรับ CNN
np.savez(OUT_PATH, X=X_mel, Y=Y_mel, genre=genres)
print("Saved mel dataset to:", OUT_PATH)

import numpy as np
import torch
from torch.utils.data import Dataset

class AutoEQDataset(Dataset):
    def __init__(self, npz_path):
        data = np.load(npz_path, allow_pickle=True)
        X = data["X"]   # (N, 128, T)
        Y = data["Y"]   # (N, 128, T)

        self.X = torch.from_numpy(X).float()
        self.Y = torch.from_numpy(Y).float()

    def __len__(self):
        return self.X.shape[0]

    def __getitem__(self, idx):
        x_mel = self.X[idx].unsqueeze(0)   # (1,128,T)
        y_mel = self.Y[idx].unsqueeze(0)
        return x_mel, y_mel

from torch.utils.data import DataLoader

npz_path = "/content/drive/MyDrive/project-music/datasets/autoeq_mel_rawmix_v1.npz"

ds = AutoEQDataset(npz_path)
dl = DataLoader(ds, batch_size=4, shuffle=True)

x_batch, y_batch = next(iter(dl))

print("x_batch:", x_batch.shape)   # (B, 1, 128, T)
print("y_batch:", y_batch.shape)

import torch
import torch.nn as nn

class AutoEQCNN(nn.Module):
    def __init__(self):
        super().__init__()

        self.body = nn.Sequential(
            nn.Conv2d(1, 16, kernel_size=3, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),

            nn.Conv2d(16, 16, kernel_size=3, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),

            nn.Conv2d(16, 16, kernel_size=3, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),

            nn.Conv2d(16, 1, kernel_size=1),
        )

    def forward(self, x):
        residual = self.body(x)
        return x + residual

import torch
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm

model = AutoEQCNN().cuda()
loss_fn = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=3e-4)

EPOCHS = 60
eps = 1e-8

baseline_loss = None
baseline_sdr  = None

for epoch in range(EPOCHS):
    model.train()
    running_loss = 0.0
    running_sdr  = 0.0
    n_batches    = 0

    loop = tqdm(dl, leave=False)

    for x, y in loop:
        x, y = x.cuda(), y.cuda()

        # forward
        pred = model(x)
        loss = loss_fn(pred, y)

        # SDR แบบกัน -inf
        num = (y ** 2).mean(dim=(1, 2, 3))
        den = ((y - pred) ** 2).mean(dim=(1, 2, 3)) + eps
        ratio = torch.clamp(num / den, min=1e-8)
        sdr_batch = 10.0 * torch.log10(ratio)
        sdr_value = sdr_batch.mean().item()

        # backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        running_sdr  += sdr_value
        n_batches    += 1

        loop.set_description(f"Epoch [{epoch+1}/{EPOCHS}]")
        loop.set_postfix(loss=loss.item(), sdr=sdr_value)

    # ถ้า dl ว่าง (กันหาร 0 เฉย ๆ — ปกติจะไม่เกิด)
    if n_batches == 0:
        print(f"Epoch {epoch+1}: no batches, skip")
        continue

    # ค่าเฉลี่ยต่อ epoch
    avg_loss = running_loss / n_batches
    avg_sdr  = running_sdr  / n_batches

    # ตั้ง baseline ที่ epoch แรก
    if epoch == 0:
        baseline_loss = avg_loss
        baseline_sdr  = avg_sdr
        loss_improve_pct = 0.0
        sdr_improve_db   = 0.0
        print(f"\n[Baseline set at epoch 1] loss={baseline_loss:.4f}, SDR={baseline_sdr:.2f} dB\n")
    else:
        loss_improve_pct = (baseline_loss - avg_loss) / baseline_loss * 100.0
        sdr_improve_db   = avg_sdr - baseline_sdr

    print(
        f"Epoch {epoch+1:02d}/{EPOCHS} | "
        f"Loss: {avg_loss:.4f} | "
        f"Improvement: {loss_improve_pct:+.2f}% | "
        f"SDR: {avg_sdr:.2f} dB | "
        f"ΔSDR: {sdr_improve_db:+.2f} dB"
    )

import os
BASE_DIR = "/content/drive/MyDrive/project-music"
MODEL_DIR = os.path.join(BASE_DIR, "models")
os.makedirs(MODEL_DIR, exist_ok=True)

MODEL_PATH = os.path.join(MODEL_DIR, "autoeq_cnn_v1.pt")

# ย้ายโมเดลไป cpu ก่อนเซฟ จะได้โหลดในเซิร์ฟเวอร์ง่าย
model_cpu = AutoEQCNN()
model_cpu.load_state_dict(model.state_dict())
torch.save(model_cpu.state_dict(), MODEL_PATH)

print("Saved model to:", MODEL_PATH)

import numpy as np
import librosa
import soundfile as sf

sr        = 44100
n_fft     = 2048
hop       = 512
n_mels    = 128

def waveform_to_mel_db(y: np.ndarray) -> np.ndarray:
    mel = librosa.feature.melspectrogram(
        y=y,
        sr=sr,
        n_fft=n_fft,
        hop_length=hop,
        n_mels=n_mels,
        power=2.0,
    )
    mel_db = librosa.power_to_db(mel, ref=np.max)
    return mel_db.astype(np.float32)

def mel_db_to_waveform(mel_db: np.ndarray, n_iter: int = 16) -> np.ndarray:
    """
    mel_db: (128, T) ในสเกล dB
    คืน: waveform mono sr=44100
    """
    mel = librosa.db_to_power(mel_db)
    stft = librosa.feature.inverse.mel_to_stft(
        mel, sr=sr, n_fft=n_fft
    )
    y = librosa.griffinlim(stft, hop_length=hop, n_iter=n_iter)
    return y.astype(np.float32)

def match_loudness_rms(y_ref: np.ndarray, y_out: np.ndarray, eps: float = 1e-8) -> np.ndarray:
    ref_rms = np.sqrt(np.mean(y_ref**2)) + eps
    out_rms = np.sqrt(np.mean(y_out**2)) + eps
    gain = ref_rms / out_rms
    return y_out * gain

def match_loudness_peak(y_ref: np.ndarray, y_out: np.ndarray, eps: float = 1e-8) -> np.ndarray:
    ref_peak = np.max(np.abs(y_ref)) + eps
    out_peak = np.max(np.abs(y_out)) + eps
    gain = ref_peak / out_peak
    return y_out * gain

def enhance_waveform(
    y_raw: np.ndarray,
    model: torch.nn.Module,
    device: str = "cuda",
    loudness_mode: str = "rms"
) -> np.ndarray:
    """
    y_raw: waveform mono (1D) sr=44100
    return: waveform ที่ผ่าน Auto-EQ แล้ว + ชดเชยความดังแล้ว
    """
    model.eval()
    with torch.no_grad():
        mel_raw = waveform_to_mel_db(y_raw)                       # (128, T)
        mel_tensor = torch.from_numpy(mel_raw).unsqueeze(0).unsqueeze(0)  # (1,1,128,T)
        mel_tensor = mel_tensor.to(device)

        mel_pred = model(mel_tensor).cpu().squeeze(0).squeeze(0).numpy()  # (128, T)

    y_enh = mel_db_to_waveform(mel_pred, n_iter=16)

    # match loudness ให้ใกล้ต้นฉบับ
    if loudness_mode == "peak":
        y_enh = match_loudness_peak(y_raw, y_enh)
    else:
        y_enh = match_loudness_rms(y_raw, y_enh)

    # กัน clip
    y_enh = np.clip(y_enh, -1.0, 1.0)
    return y_enh

def split_into_blocks(y: np.ndarray, block_sec: float = 5.0, sr: int = 44100):
    """
    y: waveform ทั้งเพลง (1D)
    block_sec: ความยาวของแต่ละ block หน่วยวินาที (เช่น 5)
    return: list ของ block (แต่ละ block เป็น np.ndarray)
    """
    block_len = int(block_sec * sr)
    blocks = []
    n = len(y)

    for start in range(0, n, block_len):
        end = start + block_len
        block = y[start:end]
        if len(block) == 0:
            continue
        blocks.append(block)

    return blocks

def enhance_long_audio(
    y_raw: np.ndarray,
    model: torch.nn.Module,
    device: str = "cuda",
    block_sec: float = 5.0,
    loudness_mode: str = "rms"
) -> np.ndarray:
    """
    รับทั้งเพลง -> หั่นเป็นบล็อก -> ส่งเข้าโมเดลทีละบล็อก -> ต่อกลับเป็นทั้งเพลง
    """
    blocks = split_into_blocks(y_raw, block_sec=block_sec, sr=sr)

    out_blocks = []
    for b in blocks:
        y_enh_block = enhance_waveform(
            b,
            model=model,
            device=device,
            loudness_mode=loudness_mode,
        )
        out_blocks.append(y_enh_block)

    if len(out_blocks) == 0:
        return y_raw

    y_enh_full = np.concatenate(out_blocks)
    return y_enh_full